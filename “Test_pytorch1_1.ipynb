{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "â€œTest_pytorch1.1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muyeby/MlInAction/blob/master/%E2%80%9CTest_pytorch1_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIiqoSI377FN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "%cd /content/drive/My Drive/\n",
        "%mkdir mywork\n",
        "%cd /content/drive/My Drive/mywork/\n",
        "%mkdir data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq3jZMmkEP7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!curl -Lo data/wiki.en.vec https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.vec\n",
        "!curl -Lo data/wiki.es.vec https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.es.vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjBucvgdH0Ow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%mkdir data/dictionaries\n",
        "!curl -Lo data/dictionaries/en-es.txt https://dl.fbaipublicfiles.com/arrival/dictionaries/en-es.txt\n",
        "!curl -Lo data/dictionaries/en-es.0-5000.txt https://dl.fbaipublicfiles.com/arrival/dictionaries/en-es.0-5000.txt\n",
        "!curl -Lo data/dictionaries/en-es.5000-6500.txt https://dl.fbaipublicfiles.com/arrival/dictionaries/en-es.5000-6500.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urZ2u52CLV_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import numpy as np\n",
        "def load_txt_embeddings(emb_path, full_vocab=True):\n",
        "    \"\"\"\n",
        "    Reload pretrained embeddings from a text file.\n",
        "    \"\"\"\n",
        "    word2id = {}\n",
        "    vectors = []\n",
        "    _emb_dim_file = 300\n",
        "    max_vocab=200000\n",
        "    \n",
        "    # load pretrained embeddings\n",
        "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i == 0:\n",
        "                split = line.split()\n",
        "                assert len(split) == 2\n",
        "            else:\n",
        "                word, vect = line.rstrip().split(' ', 1)\n",
        "                word = word.lower()\n",
        "                vect = np.fromstring(vect, sep=' ')\n",
        "                if np.linalg.norm(vect) == 0:  # avoid to have null embeddings\n",
        "                    vect[0] = 0.01\n",
        "                if word in word2id:\n",
        "                    print(\"Word {} found twice in embedding file\".format(word.encode('utf-8')))\n",
        "                else:\n",
        "                    if not vect.shape == (_emb_dim_file,):\n",
        "                        print(\"Invalid dimension (%i) for word '%s' in line %i.\"\n",
        "                                       % (vect.shape[0], word, i))\n",
        "                        continue\n",
        "                    assert vect.shape == (_emb_dim_file,), i\n",
        "                    word2id[word] = len(word2id)\n",
        "                    vectors.append(vect[None])\n",
        "            if max_vocab > 0 and len(word2id) >= max_vocab and not full_vocab:\n",
        "                break\n",
        "\n",
        "    assert len(word2id) == len(vectors)\n",
        "    print(\"Loaded %i pre-trained word embeddings.\" % len(vectors))\n",
        "\n",
        "    # compute new vocabulary / embeddings\n",
        "    id2word = {v: k for k, v in word2id.items()}\n",
        "    dico = (id2word, word2id)\n",
        "    embeddings = np.concatenate(vectors, 0)\n",
        "\n",
        "    #assert embeddings.size() == (len(dico), params.emb_dim)\n",
        "    return dico, embeddings\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL8mQ9l3PfZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_exp(seed):\n",
        "    if seed >= 0:\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed(seed)\n",
        "\n",
        "def center_embeddings(emb):\n",
        "    print('Centering the embeddings')\n",
        "    mean = emb.mean(0)\n",
        "    emb = emb-mean\n",
        "    return emb\n",
        "def norm_embeddings(emb):\n",
        "    print('Normalizing the embeddings')\n",
        "    norms = np.linalg.norm(emb,axis=1,keepdims=True)\n",
        "    norms[norms == 0] = 1\n",
        "    emb = emb / norms\n",
        "    return emb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx7J8yV_ZIn3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSx6WyiJQXgm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "d549bb0a-85ef-4267-d90d-77ee33b6a403"
      },
      "source": [
        "data_dir='/content/drive/My Drive/mywork/data'\n",
        "src_file='{}/embeddings/wiki.en.vec'.format(data_dir)\n",
        "tgt_file='{}/embeddings/wiki.es.vec'.format(data_dir)\n",
        "src_dico, src_emb = load_txt_embeddings(src_file,full_vocab=False)\n",
        "tgt_dico, tgt_emb = load_txt_embeddings(tgt_file, full_vocab=False)\n",
        "\n",
        "print(\"Centering the word embeddings...\")\n",
        "src_emb = center_embeddings(src_emb)\n",
        "tgt_emb = center_embeddings(tgt_emb)\n",
        "\n",
        "src_emb = torch.from_numpy(src_emb).float()\n",
        "tgt_emb = torch.from_numpy(tgt_emb).float()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    cuda = torch.device('cuda')\n",
        "    src_emb = src_emb.to(cuda)\n",
        "    tgt_emb = tgt_emb.to(cuda)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 200000 pre-trained word embeddings.\n",
            "Loaded 200000 pre-trained word embeddings.\n",
            "Centering the word embeddings...\n",
            "Centering the embeddings\n",
            "Centering the embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5zF-OEr1yGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AE, self).__init__()\n",
        "        self.drop1 = nn.Dropout(0.1)\n",
        "        self.map1 = nn.Linear(300, 300, bias=False)\n",
        "        self.map2 = nn.Linear(300, 300, bias=False)\n",
        "        # nn.init.eye(self.map1.weight)\n",
        "\n",
        "    def encode(self, x):\n",
        "        # x = self.drop1(x)\n",
        "        encoded = self.map1(x)\n",
        "        return encoded\n",
        "\n",
        "    def decode(self, z):\n",
        "        # decoded = F.linear(z, self.map1.weight.t(), bias=None)\n",
        "        decoded = self.map2(z)\n",
        "        return decoded\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encode(x)\n",
        "        decoded = self.decode(encoded)\n",
        "        return encoded, decoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pJh4CgU2XCS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "from timeit import default_timer as timer\n",
        "from torch.autograd import Variable\n",
        "import sys\n",
        "\n",
        "class BiAAE(object):\n",
        "    def __init__(self):\n",
        "        self.X_AE = AE()\n",
        "        self.Y_AE = AE()\n",
        "        self.nets = [self.X_AE,self.Y_AE]\n",
        "        self.loss_fn2 = torch.nn.CosineSimilarity(dim=1,eps=1e-7)\n",
        "        \n",
        "    def weights_init(self, m):          \n",
        "        if isinstance(m, torch.nn.Linear):\n",
        "            torch.nn.init.orthogonal(m.weight)\n",
        "            if m.bias is not None:\n",
        "                torch.nn.init.constant(m.bias, 0.01)\n",
        "\n",
        "    def weights_init2(self, m):  # xavier_normal     \n",
        "        if isinstance(m, torch.nn.Linear):\n",
        "            torch.nn.init.xavier_normal(m.weight)\n",
        "            if m.bias is not None:\n",
        "                torch.nn.init.constant(m.bias, 0.01)\n",
        "\n",
        "    def weights_init3(self, m):  #\n",
        "        if isinstance(m, torch.nn.Linear):\n",
        "            m.weight.data.copy_(torch.diag(torch.ones(self.params.g_input_size)))\n",
        "    \n",
        "    def init_state(self,seed=-1):\n",
        "        if torch.cuda.is_available():\n",
        "            cuda = torch.device('cuda')\n",
        "            for net in self.nets:\n",
        "                net.to(cuda)\n",
        "            self.loss_fn2 = self.loss_fn2.to(cuda)\n",
        "            \n",
        "        self.X_AE.apply(self.weights_init)  #      \n",
        "        self.Y_AE.apply(self.weights_init)  #\n",
        "        \n",
        "    def CORAL(self,source,target):\n",
        "        d = source.data.shape[1]\n",
        "        # source covariance\n",
        "        xm = torch.mean(source, 1, keepdim=True) - source\n",
        "        xc = torch.matmul(torch.transpose(xm, 0, 1), xm)\n",
        "        # target covariance\n",
        "        xmt = torch.mean(target, 1, keepdim=True) - target\n",
        "        xct = torch.matmul(torch.transpose(xmt, 0, 1), xmt)\n",
        "        # frobenius norm between source and target\n",
        "        loss = torch.mean(torch.mul((xc - xct), (xc - xct)))\n",
        "        loss = loss/(4*d*d)\n",
        "        return loss\n",
        "    \n",
        "    def get_batch_data_fast_new(self, emb_en, emb_it):\n",
        "        cuda = torch.device('cuda')\n",
        "        random_en_indices = torch.LongTensor(32).random_(75000)\n",
        "        random_it_indices = torch.LongTensor(32).random_(75000)\n",
        "        en_batch = emb_en[random_en_indices.to(cuda)]\n",
        "        it_batch = emb_it[random_it_indices.to(cuda)]\n",
        "\n",
        "        return en_batch, it_batch\n",
        "    \n",
        "    def train(self,src_dico,tgt_dico,src_emb,tgt_emb,seed):\n",
        "        \n",
        "        src_word2id = src_dico[1]\n",
        "        tgt_word2id = tgt_dico[1]\n",
        "\n",
        "        en = src_emb\n",
        "        it = tgt_emb\n",
        "        AE_optimizer = optim.SGD(filter(lambda p: p.requires_grad, list(self.X_AE.parameters()) + list(self.Y_AE.parameters())), lr=0.1)\n",
        "        \n",
        "        G_AB_recon_epochs = []\n",
        "        G_BA_recon_epochs = []\n",
        "        g_loss_epochs = []\n",
        "        coral_loss_epoches = []\n",
        "        \n",
        "        try:\n",
        "            for epoch in range(50):\n",
        "                G_AB_recon = []\n",
        "                G_BA_recon = []\n",
        "                coral_losses = []\n",
        "                g_losses = []\n",
        "                start_time = timer()\n",
        "                \n",
        "                for mini_batch in range(0, 75000 // 32):\n",
        "                    AE_optimizer.zero_grad()\n",
        "                    view_X, view_Y = self.get_batch_data_fast_new(en, it)\n",
        "                    X_Z = self.X_AE.encode(view_X)\n",
        "                    X_recon = self.X_AE.decode(X_Z)\n",
        "                    Y_fake = self.Y_AE.decode(X_Z)\n",
        "                    L_recon_X = 1.0 - torch.mean(self.loss_fn2(view_X, X_recon))\n",
        "                    \n",
        "                    Y_Z = self.Y_AE.encode(view_Y)\n",
        "                    Y_recon = self.Y_AE.decode(Y_Z)\n",
        "                    X_fake = self.X_AE.decode(Y_Z)\n",
        "                    L_recon_Y = 1.0 - torch.mean(self.loss_fn2(view_Y, Y_recon))\n",
        "                    \n",
        "                    L_coral = self.CORAL(X_Z,Y_Z)\n",
        "                    G_loss = 1.0 * (L_recon_X+L_recon_Y) + 1.0 * L_coral\n",
        "                    \n",
        "                    G_loss.backward()\n",
        "                    g_losses.append(G_loss.item())\n",
        "                    G_AB_recon.append(L_recon_X.item())\n",
        "                    G_BA_recon.append(L_recon_Y.item())\n",
        "                    coral_losses.append(L_coral.item())\n",
        "                    \n",
        "                    AE_optimizer.step()  # Only optimizes G's parameters\n",
        "                    \n",
        "                    sys.stdout.write(\"[%d/%d] ::                                     Generator Loss: %.3f \\r\" % (\n",
        "                                mini_batch, 75000 // 32, np.asscalar(np.mean(g_losses))))\n",
        "                    sys.stdout.flush()\n",
        "                    \n",
        "                G_AB_recon_epochs.append(np.asscalar(np.mean(G_AB_recon)))\n",
        "                G_BA_recon_epochs.append(np.asscalar(np.mean(G_BA_recon)))\n",
        "                coral_loss_epoches.append(np.asscalar(np.mean(coral_losses)))\n",
        "                g_loss_epochs.append(np.asscalar(np.mean(g_losses)))\n",
        "                \n",
        "                print( \"Epoch {} : Generator Loss: {:.3f}, Coral Loss: {:.3f}, Time elapsed {:.2f} mins\".\n",
        "                        format(epoch, np.asscalar(np.mean(g_losses)),np.asscalar(np.mean(coral_losses)), (timer() - start_time) / 60))\n",
        "            \n",
        "            return [G_AB_recon_epochs,G_BA_recon_epochs,coral_loss_epoches,g_loss_epochs]\n",
        "            \n",
        "        except KeyboardInterrupt:\n",
        "            print(\"Interrupted.. saving model !!!\")\n",
        "            torch.save(self.X_AE.state_dict(), 'X_model_interrupt.t7')\n",
        "            torch.save(self.Y_AE.state_dict(), 'Y_model_interrupt.t7')\n",
        "            log_file.close()\n",
        "            exit()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqDiNAJGBMu9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "237e2bad-0fda-48a6-cabb-3bf068a383a3"
      },
      "source": [
        "\n",
        "init_seed = 430\n",
        "t = BiAAE()\n",
        "initialize_exp(init_seed)\n",
        "t.init_state(seed=init_seed)\n",
        "t.train(src_dico,tgt_dico,src_emb,tgt_emb,init_seed)\n",
        "\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(range(0, len(G_AB_recon_epochs)), G_AB_recon_epochs, color='b', label='G_AB')\n",
        "plt.plot(range(0, len(G_BA_recon_epochs)), G_BA_recon_epochs, color='r', label='G_BA')\n",
        "plt.ylabel('G_recon_loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.legend()\n",
        "fig.savefig('seed_{}_G_Recon.png'.format(seed))\n",
        "            \n",
        "fig = plt.figure()\n",
        "plt.plot(range(0, len(g_loss_epochs)), g_loss_epochs, color='b', label='G_loss')\n",
        "plt.ylabel('g_loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.legend()\n",
        "fig.savefig('seed_{}_g_loss.png'.format(seed))\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(range(0, len(coral_loss_epochs)), coral_loss_epochs, color='b', label='Coral loss')\n",
        "plt.ylabel('Coral_loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.legend()\n",
        "fig.savefig(self.tune_dir + '/seed_{}_coral_loss.png'.format(seed))\n",
        "plt.close('all')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 : Generator Loss: 0.829, Coral Loss: 0.000, Time elapsed 0.29 mins\n",
            "Epoch 1 : Generator Loss: 0.312, Coral Loss: 0.000, Time elapsed 0.30 mins\n",
            "Epoch 2 : Generator Loss: 0.166, Coral Loss: 0.000, Time elapsed 0.29 mins\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}